{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Using this notebook\n",
    "\n",
    "This notebook helps you to analyze datasets and to find interesting and meaningful patterns in the data. If you are only interested in looking at an automated report outlining the most important features of your dataset, you can upload your datafile via the *dataset* variable and run the notebook. Afterwards, you can export the report as HTML and read it in a webbrowser.\n",
    "\n",
    "If you are interested in a more interactive analysis of your data, you can also adapt the parameters of the notebook to suit your needs. Each section conatins several values which can be adapted to your needs. These values are described in the code comments.\n",
    "\n",
    "Finally, if you want to go beyond an automated report and answer your own questions, you can look at the final section of the notebook and use the code examples there to generate your own figures and analysis from the data model.\n",
    "\n",
    "### Reading this report in a webbrowser\n",
    "\n",
    "This report uses several statistical methods and specific phrases and concepts from the domains of statistics and machine learning. Whenever such methods are used, a small \"Explanation\" sign at the side of the report marks a short explanation of the methods and phrases. Clicking it will reveal the explanation.\n",
    "\n",
    "You can toggle the global visibility of these explanations with a button at the top left corner of the report. The code can also be toggled with a button.\n",
    "\n",
    "All graphs are interactive and will display additional content on hover. You can get the exact values of the functions by selecting the assoziated areas in the graph. You can also move the plots around and zoom into interesting parts.\n",
    "\n",
    "### Aknowledgments\n",
    "\n",
    "This notebook is build on the MSPN implementation by Molina et.al. during the course of a bachelor thesis under the supervision of Alejandro Molina and Kristian Kersting at TU Darmstadt. The goal of this framework is to sum product networks for hybrid domains and to highlight important aspects and interesting features of a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import ba_functions as f\n",
    "import ba_plot as p\n",
    "import ba_descriptions as descr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#from tfspn.SPN import SPN\n",
    "from pprint import PrettyPrinter\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, Markdown\n",
    "from importlib import reload\n",
    "\n",
    "import plotly.plotly as py\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "from ba_functions import printmd, query, predict_proba, predict\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "pp = PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'PiecewiseLinear' has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9c24269fa708>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                               \u001b[0mindependence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindependence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                               \u001b[0misotonic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                               histogram=True)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mcategoricals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_categoricals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Studium/ProjectsKersting/SimpleSPN/src/DeepNotebooks/ba_functions.py\u001b[0m in \u001b[0;36mlearn_spn\u001b[0;34m(dataset, precision, independence, header, date, isotonic, histogram, types)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     spn = learn_mspn(data,\n\u001b[0;32m--> 140\u001b[0;31m                      \u001b[0mContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparametric_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_domains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                      \u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rdc\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                      \u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"kmeans\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Studium/ProjectsKersting/SimpleSPN/src/spn/structure/Base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, meta_types, domains, parametric_types)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparametric_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'PiecewiseLinear' has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "# path to the dataset you want to use for training\n",
    "dataset = 'example_data/titanic.csv'\n",
    "\n",
    "# the minimum number of datapoints that are included in a child of a \n",
    "# sum node\n",
    "precision = 50\n",
    "\n",
    "# the parameter which governs how strict the independece test will be\n",
    "# 1 results in all features being evaluated as independent, 0 will \n",
    "# result in no features being acccepted as truly independent\n",
    "independence = 0.1\n",
    "\n",
    "\n",
    "spn, dictionary = f.learn_spn(dataset=dataset, \n",
    "                              precision=precision, \n",
    "                              independence=independence,\n",
    "                              isotonic=False,\n",
    "                              histogram=True)\n",
    "df = pd.read_csv(dataset)\n",
    "categoricals = f.get_categoricals(spn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the model pickle file\n",
    "model_path = \"models/test.pickle\"\n",
    "\n",
    "# UNCOMMENT THE FOLLOWING LINES TO LOAD A MODEL\n",
    "#spn = SPN.from_pickle(model_path)\n",
    "#dictionary = pickle.load(model_path + \"_d\")\n",
    "\n",
    "#df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0e009f8e6e5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdescr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintroduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spn' is not defined"
     ]
    }
   ],
   "source": [
    "reload(descr)\n",
    "descr.introduction(spn)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<div class=\"cell border-box-sizing text_cell rendered\">\n",
    "<div class=\"prompt input_prompt\">Expl:</div>\n",
    "\n",
    "<div class=\"explanation\">\n",
    "The data is modeled using Sum-Product Networks. SPNs are graphical models which represent the distribution of data as sums and products of simple density functions. They exploit context specific conditional independece, which means that subsets of the data might have features which are independent of each other. For example, two features might produce two different clusters, which means that there is a dependency between them. But within these clusters, the variables are independent of each other.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('''\n",
    "---\n",
    "## General statistical evaluation''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(descr)\n",
    "descr.data_description(spn, df)\n",
    "printmd('''Below, the means and standard deviations of each feature are shown. Categorical \n",
    "features do not have a mean and a standard deviation, since they contain no ordering. Instead, \n",
    "the network returns NaN.''')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<div class=\"cell border-box-sizing text_cell rendered\">\n",
    "<div class=\"prompt input_prompt\">Expl:</div>\n",
    "\n",
    "<div class=\"explanation\">\n",
    "Continuous features are variables which have any real number within their range as a value. Discrete variables can only have integers as their values. Categorical variables are used in case the values are not ordered. They can be arbitrary but fixed strings. For example, the feature \"academic degree\" might have \"Bachelor\", \"Master\" or \"PhD\" as possible values.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr.means_table(spn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('''In the following section, the marginal distributions for each feature is shown. This \n",
    "is the distribution of each feature without knowing anything about the other values.''')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<div class=\"cell border-box-sizing text_cell rendered\">\n",
    "<div class=\"prompt input_prompt\">Expl:</div>\n",
    "\n",
    "<div class=\"explanation\">\n",
    "Marginal probabilities represent the features distribution if nothing is known about the values of the other features.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(descr)\n",
    "descr.features_shown = 2\n",
    "\n",
    "descr.show_feature_marginals(spn, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('''\n",
    "### Correlations\n",
    "\n",
    "To get a sense of how the features relate to one another, the correlation between \n",
    "them is analyzed in the next section. The correlation denotes how strongly two features are \n",
    "linked. A high correlation (close to 1 or -1) means that two features are very closely related, \n",
    "while a correlation close to 0 means that there is no linear interdependency between the features.\n",
    "\n",
    "The correlation is reported in a colored matrix, where blue denotes a negative and red denotes \n",
    "a positive correlation.''')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<div class=\"cell border-box-sizing text_cell rendered\">\n",
    "<div class=\"prompt input_prompt\">Expl:</div>\n",
    "\n",
    "<div class=\"explanation\">\n",
    "The following diagram uses three different versions of correlation. Between two continuous or discrete features, which have a mean and a variance, the Pearson correlation is calculated. This measures the degree to which there is a linear dependency between two features. If they form a strict linear function, the Pearson correlation is close to 1 or -1.\n",
    "\n",
    "Between a categorical feature and a continuous or discrete feature, the r measure is calculated. This measure represents how well the categorical value separates the variance of the continuous feature. If the groups resulting from the categorical variable are well separated, the resulting r measure is close to one.\n",
    "\n",
    "For two categorical variables, the mutual information is used. This is an entropy based measure which estimates, how much joint entropy two variables have. In essence, this estimates whether there is a functional dependency between two variables.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "descr.correlation_threshold = 0.4\n",
    "\n",
    "corr = descr.correlation_description(spn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('''The conditional distributions are the probabilities of the features, given \n",
    "a certain instance of a class. The joint probability functions of correlated variables \n",
    "are shown below to allow a more in-depth look into the dependency.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(descr)\n",
    "\n",
    "descr.correlation_threshold = 0\n",
    "descr.feature_combinations = 2\n",
    "descr.show_conditional = True\n",
    "\n",
    "descr.categorical_correlations(spn, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('''\n",
    "---\n",
    "\n",
    "## Cluster evaluation\n",
    "\n",
    "To give an impression of the data representation as a whole, the complete network graph is \n",
    "shown below. The model is a tree, with a sum node at its center. The root of the tree is shown \n",
    "in white, while the sum and product nodes are green and blue respectively. Finally, all \n",
    "leaves are represented by red nodes.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.plot_graph(spn=spn, fname='graph.png')\n",
    "display(Image(filename='graph.png', width=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('''\n",
    "The data model provides a clustering of the data points into groups in which features are \n",
    "independent. The groups extracted from the data are outlined below together with a short \n",
    "description of the data they cover. Each branch in the model represents one cluster found \n",
    "in the data model.\n",
    "\n",
    "### Description of all clusters''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# possible values: 'all', 'big', int (leading to a random sample), list of nodes to be displayed\n",
    "nodes = f.get_sorted_nodes(spn)\n",
    "\n",
    "reload(descr)\n",
    "descr.nodes = 'all'\n",
    "descr.show_node_graphs = True\n",
    "\n",
    "descr.node_introduction(spn, nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('''\n",
    "As stated above, each cluster captures a subgroup of the data. To show what variables are \n",
    "captured by which cluster, the means and variances for each feature and subgroup are plotted below. \n",
    "This highlights where the node has its focus.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr.features_shown = 2\n",
    "descr.mean_threshold = 1\n",
    "descr.variance_threshold = 2\n",
    "descr.separation_threshold = 0.3\n",
    "\n",
    "separations = descr.show_node_separation(spn, nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('''\n",
    "An analysis of the \n",
    "distribution of categorical variables is given below. If a cluster or a group of clusters \n",
    "capture a large fraction of the total likelihood of a categorical instance, they can be \n",
    "interpreted to represent this instance and the associated distribution.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr.categoricals = 'all'\n",
    "\n",
    "descr.node_categorical_description(spn, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('''\n",
    "### Correlations by cluster\n",
    "\n",
    "Finally, since each node captures different interaction between the features, it is \n",
    "interesting to look at the correlations again, this time for the seperate nodes. Shallow \n",
    "nodes are omitted, because the correlation of independent variables is always 0.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "descr.correlation_threshold = 0.3\n",
    "descr.nodes = 1\n",
    "\n",
    "descr.node_correlation(spn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('''\n",
    "---\n",
    "## Predictive data analysis\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(f)\n",
    "numerical_data, categorical_data = f.get_categorical_data(spn, df, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('''\n",
    "After the cluster description, the data model is used to predict data points. To evaluate \n",
    "the performance of the model, the misclassification rate is shown below.\n",
    "\n",
    "The classified data points are used to analyze more advanced patterns within the data, by looking\n",
    "first at the misclassified points, and then at the classification results in total.\n",
    "''')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<div class=\"cell border-box-sizing text_cell rendered\">\n",
    "<div class=\"prompt input_prompt\">Expl:</div>\n",
    "\n",
    "<div class=\"explanation\">\n",
    "To get a prediction from the SPN, the joint probability of the features together with all possible values of the target categorical are evaluated. The value with the highest probability corresponds to the prediction.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr.classify = 'all'\n",
    "\n",
    "misclassified, data_dict = descr.classification(spn, numerical_data, categorical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('''\n",
    "Below, the misclassified examples are explained using the clusters they are most assoiciated with.\n",
    "For each instance, those clusters which form 90 % of the prediction are reported together eith the\n",
    "representatives of these clusters.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Only set use_shapley to true if you have a really powerful machine\n",
    "reload(descr)\n",
    "reload(p)\n",
    "\n",
    "descr.use_shapley = False\n",
    "descr.shapley_sample_size = 1\n",
    "descr.misclassified_explanations = 1\n",
    "\n",
    "descr.describe_misclassified(spn, dictionary, misclassified, data_dict, numerical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('''\n",
    "### Information gain through features\n",
    "\n",
    "The following graphs highlight the relative importance of different features for a \n",
    "classification. It can show how different classes are predicted. For continuous and\n",
    "discrete features, a high positvie or negative importance shows that changing this features\n",
    "value positive or negative increases the predictions certainty.\n",
    "\n",
    "For categorical values, positive and negative values highlight whether changing or keeping\n",
    "this categorical value increases or decreasies the predictive certainty.\n",
    "''')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<div class=\"cell border-box-sizing text_cell rendered\">\n",
    "<div class=\"prompt input_prompt\">Expl:</div>\n",
    "\n",
    "<div class=\"explanation\">\n",
    "The method used to calculate the influence of a feature on a prediction is called influence vectors and is based on the gradient of the probability function. For each data point, the local gradient of the decision function describes what values need to be changed to get a higher certainty for the prediction. This allows us to get a sense of how the features interact. If, for example, only one feature shows a significant change in the prediction funciton, this feature is locally most important for the prediction.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(descr)\n",
    "reload(p)\n",
    "\n",
    "descr.explanation_vector_threshold = 0\n",
    "descr.explanation_vector_classes = None\n",
    "descr.explanation_vectors_show = 'all'\n",
    "\n",
    "expl_vectors = descr.explanation_vector_description(spn, dictionary, data_dict, categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd(\n",
    "'''\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(descr)\n",
    "descr.print_conclusion(spn, dictionary, corr, nodes, separations, expl_vectors)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<div class=\"examples\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dive into the data\n",
    "\n",
    "Use the Facets Interface to visualize data on your own. You can either load the dataset itself, or show the data as predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load UCI census and convert to json for sending to the visualization\n",
    "import pandas as pd\n",
    "df = pd.read_csv(dataset)\n",
    "jsonstr = df.to_json(orient='records')\n",
    "\n",
    "# Display the Dive visualization for this data\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "HTML_TEMPLATE = \"\"\"<link rel=\"import\" href=\"/nbextensions/facets-dist/facets-jupyter.html\">\n",
    "        <facets-dive id=\"elem\" height=\"600\"></facets-dive>\n",
    "        <script>\n",
    "          var data = {jsonstr};\n",
    "          document.querySelector(\"#elem\").data = data;\n",
    "        </script>\"\"\"\n",
    "html = HTML_TEMPLATE.format(jsonstr=jsonstr)\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your own queries\n",
    "\n",
    "This notebook enables you to add your own analysis to the above. Maybe you are interested in drilling down into specific subclusters of the data, or you want to predict additional datapoint not represented in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get samples to predict\n",
    "data_point = numerical_data[1:2]\n",
    "# get the probability from the models joint probability function\n",
    "proba = query(spn, data_point)\n",
    "\n",
    "\n",
    "printmd(data_point)\n",
    "printmd(query(spn, data_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also predict the probability of several data points at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_point = numerical_data[0:3]\n",
    "proba = query(spn, data_point)\n",
    "\n",
    "printmd(data_point)\n",
    "printmd(proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in predicting the probability of a class variable, you can use the following functions. The first one returns the probabilities of all possible class instances, the second one returns the instance with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probabilities of all classes\n",
    "proba = predict_proba(spn, categoricals[0], data_point)\n",
    "# get the most likely class\n",
    "predictions = predict(spn, categoricals[0], data_point)\n",
    "\n",
    "for prob in proba:\n",
    "    printmd(prob)\n",
    "printmd(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code provides a more complicated example, showcasing how the data model can be used more in depth for a logistic regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the first continuous feature for a regression task\n",
    "non_categorical = [i for i, name in enumerate(spn.featureNames) if name != 'categorical'][0]\n",
    "\n",
    "# get sample data points\n",
    "instances = numerical_data[0:5].copy()\n",
    "\n",
    "# generate evidence for the regression. All features are used as predictors\n",
    "evidence = instances.copy()\n",
    "evidence[:,non_categorical] = np.nan\n",
    "\n",
    "# get the prediction using the most probable explanation from the model\n",
    "prediction = spn.root.mpe_eval(evidence)\n",
    "\n",
    "#print the output\n",
    "printmd('Predicted values: {}'.format(prediction[1][:,non_categorical]))\n",
    "printmd('Real instances: {}'.format(instances[:,non_categorical]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
